{"metadata":{"colab":{"collapsed_sections":[],"name":"chapter04_getting-started-with-neural-networks.i","private_outputs":false,"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n\n**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n\nThis notebook was generated for TensorFlow 2.6.","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"# Timing Code\nimport pytz\nimport time\nimport datetime\n\nt0 = time.time()\nnow_utc = datetime.datetime.utcnow()\nist_tz = pytz.timezone('Asia/Kolkata')\nnow_ist = now_utc.astimezone(ist_tz)\nprint(\"Current time in IST:\", now_ist.strftime(\"%Y-%m-%d %H:%M:%S %Z%z\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting started with neural networks: Classification and regression","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"## Classifying movie reviews: A binary classification example","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"### The IMDB dataset","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**Loading the IMDB dataset**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"from tensorflow.keras.datasets import imdb\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=10000)","metadata":{"colab_type":"code"},"execution_count":1,"outputs":[{"name":"stderr","output_type":"stream","text":"2023-03-09 21:52:09.246206: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"},{"name":"stdout","output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n\n17464789/17464789 [==============================] - 4s 0us/step\n"}]},{"cell_type":"code","source":"train_data[0]","metadata":{"colab_type":"code"},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":["[1,\n"," 14,\n"," 22,\n"," 16,\n"," 43,\n"," 530,\n"," 973,\n"," 1622,\n"," 1385,\n"," 65,\n"," 458,\n"," 4468,\n"," 66,\n"," 3941,\n"," 4,\n"," 173,\n"," 36,\n"," 256,\n"," 5,\n"," 25,\n"," 100,\n"," 43,\n"," 838,\n"," 112,\n"," 50,\n"," 670,\n"," 2,\n"," 9,\n"," 35,\n"," 480,\n"," 284,\n"," 5,\n"," 150,\n"," 4,\n"," 172,\n"," 112,\n"," 167,\n"," 2,\n"," 336,\n"," 385,\n"," 39,\n"," 4,\n"," 172,\n"," 4536,\n"," 1111,\n"," 17,\n"," 546,\n"," 38,\n"," 13,\n"," 447,\n"," 4,\n"," 192,\n"," 50,\n"," 16,\n"," 6,\n"," 147,\n"," 2025,\n"," 19,\n"," 14,\n"," 22,\n"," 4,\n"," 1920,\n"," 4613,\n"," 469,\n"," 4,\n"," 22,\n"," 71,\n"," 87,\n"," 12,\n"," 16,\n"," 43,\n"," 530,\n"," 38,\n"," 76,\n"," 15,\n"," 13,\n"," 1247,\n"," 4,\n"," 22,\n"," 17,\n"," 515,\n"," 17,\n"," 12,\n"," 16,\n"," 626,\n"," 18,\n"," 2,\n"," 5,\n"," 62,\n"," 386,\n"," 12,\n"," 8,\n"," 316,\n"," 8,\n"," 106,\n"," 5,\n"," 4,\n"," 2223,\n"," 5244,\n"," 16,\n"," 480,\n"," 66,\n"," 3785,\n"," 33,\n"," 4,\n"," 130,\n"," 12,\n"," 16,\n"," 38,\n"," 619,\n"," 5,\n"," 25,\n"," 124,\n"," 51,\n"," 36,\n"," 135,\n"," 48,\n"," 25,\n"," 1415,\n"," 33,\n"," 6,\n"," 22,\n"," 12,\n"," 215,\n"," 28,\n"," 77,\n"," 52,\n"," 5,\n"," 14,\n"," 407,\n"," 16,\n"," 82,\n"," 2,\n"," 8,\n"," 4,\n"," 107,\n"," 117,\n"," 5952,\n"," 15,\n"," 256,\n"," 4,\n"," 2,\n"," 7,\n"," 3766,\n"," 5,\n"," 723,\n"," 36,\n"," 71,\n"," 43,\n"," 530,\n"," 476,\n"," 26,\n"," 400,\n"," 317,\n"," 46,\n"," 7,\n"," 4,\n"," 2,\n"," 1029,\n"," 13,\n"," 104,\n"," 88,\n"," 4,\n"," 381,\n"," 15,\n"," 297,\n"," 98,\n"," 32,\n"," 2071,\n"," 56,\n"," 26,\n"," 141,\n"," 6,\n"," 194,\n"," 7486,\n"," 18,\n"," 4,\n"," 226,\n"," 22,\n"," 21,\n"," 134,\n"," 476,\n"," 26,\n"," 480,\n"," 5,\n"," 144,\n"," 30,\n"," 5535,\n"," 18,\n"," 51,\n"," 36,\n"," 28,\n"," 224,\n"," 92,\n"," 25,\n"," 104,\n"," 4,\n"," 226,\n"," 65,\n"," 16,\n"," 38,\n"," 1334,\n"," 88,\n"," 12,\n"," 16,\n"," 283,\n"," 5,\n"," 16,\n"," 4472,\n"," 113,\n"," 103,\n"," 32,\n"," 15,\n"," 16,\n"," 5345,\n"," 19,\n"," 178,\n"," 32]"]},"metadata":{}}]},{"cell_type":"code","source":"train_labels[0]","metadata":{"colab_type":"code"},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{}}]},{"cell_type":"code","source":"max([max(sequence) for sequence in train_data])","metadata":{"colab_type":"code"},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":["9999"]},"metadata":{}}]},{"cell_type":"markdown","source":"**Decoding reviews back to text**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"word_index = imdb.get_word_index()\nreverse_word_index = dict(\n    [(value, key) for (key, value) in word_index.items()])\ndecoded_review = \" \".join(\n    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])","metadata":{"colab_type":"code"},"execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n\n1641221/1641221 [==============================] - 0s 0us/step\n"}]},{"cell_type":"markdown","source":"### Preparing the data","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**Encoding the integer sequences via multi-hot encoding**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"import numpy as np\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        for j in sequence:\n            results[i, j] = 1.\n    return results\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train[0]","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = np.asarray(train_labels).astype(\"float32\")\ny_test = np.asarray(test_labels).astype(\"float32\")","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building your model","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**Model definition**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compiling the model**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"model.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validating your approach","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**Setting aside a validation set**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"x_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training your model**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history.history\nhistory_dict.keys()","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting the training and validation loss**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nhistory_dict = history.history\nloss_values = history_dict[\"loss\"]\nval_loss_values = history_dict[\"val_loss\"]\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting the training and validation accuracy**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"plt.clf()\nacc = history_dict[\"accuracy\"]\nval_acc = history_dict[\"val_accuracy\"]\nplt.plot(epochs, acc, \"bo\", label=\"Training acc\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\nplt.title(\"Training and validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Retraining a model from scratch**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"model = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)\nresults = model.evaluate(x_test, y_test)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using a trained model to generate predictions on new data","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"model.predict(x_test)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Further experiments","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"### Wrapping up","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"## Classifying newswires: A multiclass classification example","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"### The Reuters dataset","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**Loading the Reuters dataset**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"from tensorflow.keras.datasets import reuters\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n    num_words=10000)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_data)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[10]","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decoding newswires back to text**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"word_index = reuters.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_newswire = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in\n    train_data[0]])","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels[10]","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparing the data","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**Encoding the input data**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"x_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encoding the labels**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"def to_one_hot(labels, dimension=46):\n    results = np.zeros((len(labels), dimension))\n    for i, label in enumerate(labels):\n        results[i, label] = 1.\n    return results\ny_train = to_one_hot(train_labels)\ny_test = to_one_hot(test_labels)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\ny_train = to_categorical(train_labels)\ny_test = to_categorical(test_labels)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building your model","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**Model definition**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"model = keras.Sequential([\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(46, activation=\"softmax\")\n])","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compiling the model**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"model.compile(optimizer=\"rmsprop\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validating your approach","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**Setting aside a validation set**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"x_val = x_train[:1000]\npartial_x_train = x_train[1000:]\ny_val = y_train[:1000]\npartial_y_train = y_train[1000:]","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training the model**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting the training and validation loss**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"loss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting the training and validation accuracy**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"plt.clf()\nacc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\nplt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Retraining a model from scratch**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"model = keras.Sequential([\n  layers.Dense(64, activation=\"relu\"),\n  layers.Dense(64, activation=\"relu\"),\n  layers.Dense(46, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(x_train,\n          y_train,\n          epochs=9,\n          batch_size=512)\nresults = model.evaluate(x_test, y_test)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\ntest_labels_copy = copy.copy(test_labels)\nnp.random.shuffle(test_labels_copy)\nhits_array = np.array(test_labels) == np.array(test_labels_copy)\nhits_array.mean()","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating predictions on new data","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"predictions = model.predict(x_test)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions[0].shape","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sum(predictions[0])","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argmax(predictions[0])","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A different way to handle the labels and the loss","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"y_train = np.array(train_labels)\ny_test = np.array(test_labels)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=\"rmsprop\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The importance of having sufficiently large intermediate layers","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**A model with an information bottleneck**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"model = keras.Sequential([\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(4, activation=\"relu\"),\n    layers.Dense(46, activation=\"softmax\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(partial_x_train,\n          partial_y_train,\n          epochs=20,\n          batch_size=128,\n          validation_data=(x_val, y_val))","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Further experiments","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"### Wrapping up","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"## Predicting house prices: A regression example","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"### The Boston Housing Price dataset","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**Loading the Boston housing dataset**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"from tensorflow.keras.datasets import boston_housing\n(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.shape","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_targets","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparing the data","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**Normalizing the data**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"mean = train_data.mean(axis=0)\ntrain_data -= mean\nstd = train_data.std(axis=0)\ntrain_data /= std\ntest_data -= mean\ntest_data /= std","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building your model","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**Model definition**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"def build_model():\n    model = keras.Sequential([\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(64, activation=\"relu\"),\n        layers.Dense(1)\n    ])\n    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n    return model","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validating your approach using K-fold validation","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"**K-fold validation**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"k = 4\nnum_val_samples = len(train_data) // k\nnum_epochs = 100\nall_scores = []\nfor i in range(k):\n    print(f\"Processing fold #{i}\")\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n    model = build_model()\n    model.fit(partial_train_data, partial_train_targets,\n              epochs=num_epochs, batch_size=16, verbose=0)\n    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n    all_scores.append(val_mae)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_scores","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(all_scores)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Saving the validation logs at each fold**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"num_epochs = 500\nall_mae_histories = []\nfor i in range(k):\n    print(f\"Processing fold #{i}\")\n    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate(\n        [train_data[:i * num_val_samples],\n         train_data[(i + 1) * num_val_samples:]],\n        axis=0)\n    partial_train_targets = np.concatenate(\n        [train_targets[:i * num_val_samples],\n         train_targets[(i + 1) * num_val_samples:]],\n        axis=0)\n    model = build_model()\n    history = model.fit(partial_train_data, partial_train_targets,\n                        validation_data=(val_data, val_targets),\n                        epochs=num_epochs, batch_size=16, verbose=0)\n    mae_history = history.history[\"val_mae\"]\n    all_mae_histories.append(mae_history)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Building the history of successive mean K-fold validation scores**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"average_mae_history = [\n    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting validation scores**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.show()","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting validation scores, excluding the first 10 data points**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"truncated_mae_history = average_mae_history[10:]\nplt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation MAE\")\nplt.show()","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training the final model**","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"model = build_model()\nmodel.fit(train_data, train_targets,\n          epochs=130, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(test_data, test_targets)","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_mae_score","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating predictions on new data","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"predictions = model.predict(test_data)\npredictions[0]","metadata":{"colab_type":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Wrapping up","metadata":{"colab_type":"text"}},{"cell_type":"markdown","source":"## Summary","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"# End Timing Code \nprint(\"Took:{0:.2f}s\".format(time.time() - t0))\nnow_utc = datetime.datetime.utcnow()\nist_tz = pytz.timezone('Asia/Kolkata')\nnow_ist = now_utc.astimezone(ist_tz)\nprint(\"Current time in IST:\", now_ist.strftime(\"%Y-%m-%d %H:%M:%S %Z%z\"))","metadata":{},"execution_count":null,"outputs":[]}]}